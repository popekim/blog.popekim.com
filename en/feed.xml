<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blog.popekim.com/en/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.popekim.com/en/" rel="alternate" type="text/html" /><updated>2026-02-27T00:30:58+00:00</updated><id>https://blog.popekim.com/en/feed.xml</id><title type="html">PPMC</title><subtitle>Pope Kim&apos;s Blog</subtitle><author><name>Pope Kim</name></author><entry><title type="html">How We Maintain a Monorepo, and Why DLL Boundaries Matter More</title><link href="https://blog.popekim.com/en/2026/02/18/monorepo-architecture.html" rel="alternate" type="text/html" title="How We Maintain a Monorepo, and Why DLL Boundaries Matter More" /><published>2026-02-18T00:00:00+00:00</published><updated>2026-02-18T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2026/02/18/monorepo-architecture</id><content type="html" xml:base="https://blog.popekim.com/en/2026/02/18/monorepo-architecture.html"><![CDATA[<p>The company I run fundamentally adopts a <strong>Monorepo</strong> approach. Our folder structure is therefore not designed to "make the code look neat," but rather based on <strong>how we manage dependencies and reuse code properly</strong>.</p>

<p>Many people debate whether to organize by feature or by domain. I approach this from a slightly different perspective.</p>

<!--more-->

<p>The core is not folders, but <strong>project (.csproj) boundaries</strong> ‚Äî in other words, <strong>DLL-level separation</strong>.</p>

<h2 id="level-0---product-level-separation">Level 0 - Product-Level Separation</h2>

<p>First, we clearly define: "Which product does this code belong to?"</p>

<ul>
  <li><strong>Academy</strong>: Code related to POCU Academy</li>
  <li><strong>ProctoredExamService</strong>: Online exam proctoring service</li>
  <li><strong>Engine</strong>: Code shared across multiple products (effectively internal middleware)</li>
</ul>

<p>At this level, product boundaries are already clear. If code ownership becomes ambiguous, the overall structure starts to destabilize.</p>

<h2 id="level-1---project-csproj-level">Level 1 - Project (.csproj) Level</h2>

<p>This is the most important layer.</p>

<p>Each product contains multiple <code>.csproj</code> files.</p>

<p>For example:</p>

<ul>
  <li><code>Academy.Services</code></li>
  <li><code>Academy.Buildfarm</code></li>
  <li><code>Shop</code> (the web app users directly interact with)</li>
</ul>

<p>Level 1 is not just a folder. It represents a <strong>DLL boundary</strong>.</p>

<h3 id="the-real-reason-i-split-at-level-1">The Real Reason I Split at Level 1</h3>

<p>Is it feature separation? Domain separation? No.</p>

<p>It is for <strong>proper dependency management and access control of shared code</strong>.</p>

<h2 id="operational-approach">Operational Approach</h2>

<h3 id="1-app-specific-code">1) App-Specific Code</h3>

<p>Code used only by a specific app remains inside that project. The internal folder structure is freely organized based on team agreement.</p>

<p>In practice, folder structure affects development efficiency by perhaps 10%.</p>

<p>Most navigation happens through:</p>

<ul>
  <li>Go to Definition</li>
  <li>Search All References</li>
  <li>Global Search (Ctrl + Shift + F)</li>
  <li>Tracing through build errors</li>
</ul>

<p>This is far more efficient than manually navigating folders.</p>

<h3 id="2-shared-code">2) Shared Code</h3>

<p>When code is required by two or more apps, we move it into a shared library such as <code>Academy.Libs</code>.</p>

<p>Namespaces are automatically determined by folder structure.</p>

<p>For example:</p>

<pre><code>Academy.Libs/Services/Order/OrderService.cs
-&gt; namespace Academy.Services.Order
</code></pre>

<p>If we later extract this folder into a dedicated <code>Academy.Services.csproj</code>, the namespace remains unchanged.</p>

<p>We simply reconnect project references and everything works.</p>

<p>This is critical. We must be able to insert code quickly and extract it into a standalone module when needed.</p>

<h3 id="3-further-separation-when-necessary">3) Further Separation When Necessary</h3>

<p>As shared code grows, we separate it into dedicated libraries.</p>

<p>For example:</p>

<ul>
  <li><code>Academy.Entities</code>: ORM entities + query extensions</li>
  <li><code>Academy.Services</code>: Shared service logic</li>
</ul>

<p>Here, <strong>access control strategy</strong> becomes crucial.</p>

<h2 id="access-control-and-collaboration-rules">Access Control and Collaboration Rules</h2>

<p>Many classes inside <code>Academy.Entities</code> and <code>Academy.Services</code> are marked as <code>internal</code>.</p>

<p>We selectively grant access via <code>InternalsVisibleTo</code> only when necessary.</p>

<p>Why?</p>

<ul>
  <li>ORM entities</li>
  <li>Core service logic</li>
  <li>Performance-critical components</li>
</ul>

<p>Allowing unrestricted modification by junior developers significantly increases the risk of production issues.</p>

<h3 id="actual-collaboration-rules">Actual Collaboration Rules</h3>

<ul>
  <li><strong>Shop Project</strong>
    <ul>
      <li>Anyone can modify</li>
      <li>Merge to main without mandatory review</li>
    </ul>
  </li>
  <li><strong>Academy.Services / Academy.Entities</strong>
    <ul>
      <li>Only senior developers may modify</li>
      <li>Juniors require senior review before merging</li>
    </ul>
  </li>
</ul>

<p>If folder structure contributes 10% to efficiency, structured access control contributes far more.</p>

<p>This is what truly ensures productivity and stability.</p>

<h2 id="separation-for-nuget-size-optimization">Separation for NuGet Size Optimization</h2>

<p>Sometimes we split libraries purely for deployment reasons.</p>

<p>For example, placing the <code>clang</code> toolset inside <code>Academy.Libs</code> would increase every application's deployment size by hundreds of megabytes.</p>

<p>So we separate it into its own DLL.</p>

<p>Again, this is not about feature or domain separation ‚Äî it is about deployment strategy and dependency control.</p>

<h2 id="level-2---internal-project-folders">Level 2 - Internal Project Folders</h2>

<p>This layer is flexible.</p>

<p>Typical folders:</p>

<ul>
  <li>Services</li>
  <li>Models</li>
  <li>Entities</li>
  <li>TransferData</li>
</ul>

<p>My usual convention:</p>

<ul>
  <li>DTOs ‚Üí <code>TransferData</code></li>
  <li>ViewModels ‚Üí <code>Models</code></li>
  <li>Database entities ‚Üí <code>Entities</code></li>
</ul>

<p>This structure changes frequently:</p>

<ul>
  <li>When features expand</li>
  <li>When concepts are redefined</li>
  <li>When I make mistakes</li>
</ul>

<p>Modern C# IDEs automatically update namespaces and references, so moving files is low risk.</p>

<p>Move files, compile, verify. Simple.</p>

<h2 id="how-we-actually-navigate-the-codebase">How We Actually Navigate the Codebase</h2>

<p>In reality:</p>

<ul>
  <li>90% of navigation happens via IDE features</li>
  <li>10% through manual folder traversal</li>
</ul>

<p>My philosophy is this:</p>

<blockquote>
  <p>Folder structure is merely a management tool.<br />
What truly matters is project-level dependency management and access control.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<ol>
  <li>Level 0 and 1 must be rigorously structured</li>
  <li>Level 1 is about DLL boundaries, not feature/domain labels</li>
  <li>Level 2 can remain flexible</li>
  <li>IDE navigation + compilation drive maintainability</li>
  <li>Eliminating duplication and enforcing access control ensures long-term stability</li>
</ol>

<p>In one sentence:</p>

<blockquote>
  <p>Insert quickly, extract cleanly when needed ‚Äî and enforce strong access control on top.</p>
</blockquote>

<p>This is how the company I run manages its monorepo architecture.</p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="dev" /><category term="git" /><category term="best-practice" /><category term="simplicity" /><summary type="html"><![CDATA[The company I run fundamentally adopts a Monorepo approach. Our folder structure is therefore not designed to "make the code look neat," but rather based on how we manage dependencies and reuse code properly. Many people debate whether to organize by feature or by domain. I approach this from a slightly different perspective.]]></summary></entry><entry><title type="html">The Repeating 3-Week Pattern: What I Learned by Finishing the Last Course Myself</title><link href="https://blog.popekim.com/en/2025/11/21/repeating-3week-pattern.html" rel="alternate" type="text/html" title="The Repeating 3-Week Pattern: What I Learned by Finishing the Last Course Myself" /><published>2025-11-21T00:00:00+00:00</published><updated>2025-11-21T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/11/21/repeating-3week-pattern</id><content type="html" xml:base="https://blog.popekim.com/en/2025/11/21/repeating-3week-pattern.html"><![CDATA[<p><a href="https://pocu.academy">POCU video lectures</a> come with transcripts that someone already cleaned up once. The content is mostly accurate, but the line breaks and punctuation are all over the place. To use them in an automated pipeline, they need to be reorganized. One student had been helping me with that work for several years on a part-time basis.</p>

<p>He worked on multiple courses, and strangely, the pattern was always the same.</p>

<!--more-->

<p>He did exactly 3 weeks of content very well. Then he disappeared.</p>

<p>A few months later he would reappear, apologize, finish another 3 weeks, and disappear again. It did not matter which course it was; the pattern repeated every single time.</p>

<p>And now that I think about it, it was not because he was lazy. It is simply that the work itself was the kind that wears people out.</p>

<p>I finally understood this very clearly when I tried doing it myself.</p>

<h2 id="the-last-course-and-an-8-month-gap">The last course, and an 8-month gap</h2>

<p>This time, the task he received was truly the last remaining course. Finishing this one would complete a multi-year transcript improvement project. As usual, he completed the first 3 weeks perfectly.</p>

<p>Then he disappeared again. Three weeks later he showed up, extremely apologetic. I could feel his guilt and pressure right through the messages. That heavy feeling of "I know I should do it, but I just cannot get myself to do it."</p>

<p>And after finishing another 3 weeks, he disappeared again. This time, the gap was especially long. About 8 months.</p>

<p>The weight of the guilt he had shown before felt too heavy. I realized it would not be good for either of us to drag this out any longer.</p>

<p>So I decided to finish the last course myself.</p>

<h2 id="and-then-i-also-got-stuck-exactly-at-the-3-week-mark">And then‚Ä¶ I also got stuck exactly at the 3-week mark</h2>

<p>Once I started working on it, I realized something funny. I also stopped right at the 3-week mark.</p>

<p>The beginning went smoothly. Cleaning up sentences felt satisfying, and the progress was visible. I thought, "This is going pretty well." The speed was good too.</p>

<p>But the moment I crossed the 3-week line, suddenly nothing was left except endless repetition. The interest vanished. My motivation and energy drained at the same time. I even started disliking myself for sitting at the desk.</p>

<p>That is when I realized:</p>

<blockquote>
  <p>"Ah‚Ä¶ this must have been exactly what he felt every time he stopped at 3 weeks."</p>
</blockquote>

<p>Honestly, I had quietly wondered, "Why can he never finish it?" But once I experienced the same block, I had no right to judge him.</p>

<p>To be even more honest, he was better than me. At least he had the courage to apologize and come back after a few months. I was just annoyed at myself.</p>

<h2 id="to-keep-going-i-started-stacking-reasons">To keep going, I started stacking reasons</h2>

<p>Even so, this time I could not run away. This was the final course. If I dropped it, no one else would pick it up.</p>

<p>So I started collecting reasons to pull myself back to the desk.</p>

<h3 id="1-finishing-this-opens-the-automation-pipeline">1) Finishing this opens the automation pipeline</h3>
<p>It is annoying now, but finishing this task will reduce future annoyance hundreds of times over.</p>

<h3 id="2-i-cannot-enjoy-fun-projects-if-this-is-left-unfinished">2) I cannot enjoy fun projects if this is left unfinished</h3>
<p>New lectures, system development, interesting experiments‚Ä¶ I want to do all of them, but this unfinished task keeps weighing on my mind.</p>

<h3 id="3-and-coincidentally-i-was-sick-for-a-few-days">3) And coincidentally, I was sick for a few days</h3>
<p>Strangely, mentally intensive work becomes impossible when I feel unwell, but simple repetitive work actually becomes easier. On the day my condition was the worst, this task progressed the fastest.</p>

<p>Holding onto these reasons, I decided to switch into machine mode again.</p>

<h2 id="in-the-end-i-became-a-machine-once-more">In the end, I became a machine once more</h2>

<p>People have often told me since I was young that I seem like a machine. I could never tell whether it was a compliment or an insult. But working on this again reminded me why they say that.</p>

<p>I am someone who suppresses emotions well, and once I decide to do something boring and repetitive, I push through it until the end. Emotions can be processed later.</p>

<p>So I finished the rest in just 5 days.</p>

<p>Truly like a machine.</p>

<h2 id="if-i-feel-less-pain-doing-something-others-find-painful-maybe-that-is-my-job">"If I feel less pain doing something others find painful, maybe that is my job"</h2>

<p>After saving the final file, I remembered something from the book Atomic Habits. I am paraphrasing, but the idea was this:</p>

<blockquote>
  <p>What matters is not whether you like the work, but whether you feel the pain less than others do. If you can endure something that others struggle with, that might be the work you are meant to do.</p>
</blockquote>

<p>That was exactly the case with this transcript-cleaning task.</p>

<p>For many people, this kind of work is unbearably boring and painful. But I am, unfortunately, someone who feels that pain a little less. So I was the one who ended up finishing this long project.</p>

<p>It is not something to brag about. It is simply how I am wired. You could call it persistence, or stubbornness, or if we are being very honest‚Ä¶ just being a bit rigid.</p>

<p>Still, someone had to finish this work, and it happened to be me.</p>

<p>And now, finally, I can move on to the fun things with a lighter heart.</p>]]></content><author><name>Pope Kim</name></author><category term="personal" /><category term="dev-diary" /><category term="dev" /><category term="pocu" /><category term="simplicity" /><category term="time-management" /><category term="life-advice" /><category term="wisdom" /><category term="book" /><summary type="html"><![CDATA[POCU video lectures come with transcripts that someone already cleaned up once. The content is mostly accurate, but the line breaks and punctuation are all over the place. To use them in an automated pipeline, they need to be reorganized. One student had been helping me with that work for several years on a part-time basis. He worked on multiple courses, and strangely, the pattern was always the same.]]></summary></entry><entry><title type="html">Old New Music: How Can I Make You Leave Her?</title><link href="https://blog.popekim.com/en/2025/11/03/new-old-music-leave-her.html" rel="alternate" type="text/html" title="Old New Music: How Can I Make You Leave Her?" /><published>2025-11-03T00:00:00+00:00</published><updated>2025-11-03T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/11/03/new-old-music-leave-her</id><content type="html" xml:base="https://blog.popekim.com/en/2025/11/03/new-old-music-leave-her.html"><![CDATA[<p>I realized I never shared this song on my blog. It's a track I released on April Fool's Day back in 2015.</p>

<p>The title is "How Can I Make You Leave Her?"</p>

<!--more-->

<p>People who heard it were split into two very different groups: Some said it sounded hopeful, while others said it felt depressing.</p>

<p>So what was I trying to convey?</p>]]></content><author><name>Pope Kim</name></author><category term="music" /><category term="music" /><category term="my-work" /><category term="youtube" /><summary type="html"><![CDATA[I realized I never shared this song on my blog. It's a track I released on April Fool's Day back in 2015. The title is "How Can I Make You Leave Her?"]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://img.youtube.com/vi/92GCdByc9Zk/maxresdefault.jpg" /><media:content medium="image" url="https://img.youtube.com/vi/92GCdByc9Zk/maxresdefault.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stripe, KRW Local Payments, and the Never-Ending DCC Problem</title><link href="https://blog.popekim.com/en/2025/11/02/stripe-krw-local-payment-dcc.html" rel="alternate" type="text/html" title="Stripe, KRW Local Payments, and the Never-Ending DCC Problem" /><published>2025-11-02T00:00:00+00:00</published><updated>2025-11-02T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/11/02/stripe-krw-local-payment-dcc</id><content type="html" xml:base="https://blog.popekim.com/en/2025/11/02/stripe-krw-local-payment-dcc.html"><![CDATA[<p>Every time you pay on an overseas website, you've probably seen that little popup asking if you'd like to pay in KRW or USD. That's DCC ‚Äî Dynamic Currency Conversion. It sounds convenient, but in reality it almost always costs you more. The merchant or payment processor uses its own exchange rate and adds a markup on top. So, in most cases, it's much cheaper to just pay in the local currency (USD, EUR, etc.). Paying in KRW on a foreign site basically means paying extra for nothing.</p>

<p>Stripe once planned to enter the Korean market ‚Äî they even started hiring ‚Äî but eventually went quiet. Maybe it's because of Korea's complex financial regulations or the already-saturated PG market, but things have moved slower than expected. Still, starting in late 2024, Stripe's documentation finally began mentioning support for Korean local payments. According to their docs, Checkout and Elements now support Korean credit and debit cards, as well as popular local wallets like Naver Pay, Kakao Pay, Samsung Pay, and Payco. Sounds like good news, right? But once you actually try it, it's clear that ‚Äúit's not as simple as it sounds.‚Äù Many of those options require your Stripe account to be registered under a Korean entity or a U.S.-based legal entity. So technically it's ‚Äúavailable,‚Äù but in practice, not fully open yet.</p>

<p>From <a href="https://pocu.academy/en">POCU Academy</a>'s perspective, it's kind of a funny situation. We currently sell courses in two ways: the <a href="https://pocu.academy/en/Courses">regular semester (full courses) are sold directly through our own website</a> operated by a Canadian corporation, and the <a href="https://pocu-en.teachable.com/l/products">on-demand video lectures are sold through the U.S. platform Teachable</a>. Both use Stripe for payments. And both still have DCC attached. At first, I hoped that since Stripe started supporting Korean payments, maybe DCC would finally disappear ‚Äî but nope, it's still there. For <a href="https://pocu.academy/en">POCU Academy</a>, which runs under a Canadian entity, that's understandable. But for a U.S.-based company like Teachable, it was surprising. When I looked more closely at the receipt, I found that Teachable's Stripe account is registered under a Netherlands entity. So it's using an EU-based Stripe setup, meaning it doesn't yet utilize the U.S. Stripe infrastructure that supports Korean local payments. I think Teachable's parent company might actually be based in the Netherlands? If that's true, it probably simplifies handling payments across EU countries, even if it complicates things for Korea.</p>

<p>Stripe usually rolls out new features in this order: U.S. ‚Üí Europe ‚Üí Canada. So maybe Teachable will get the DCC-free benefit first, and then POCU Academy will follow. The reason European and Canadian entities don't yet have access is that each country's banking and card networks have to be interconnected. For instance, when a Korean customer pays in KRW, Stripe has to accept that payment through a Korean acquiring bank, then later settle it in the merchant's home currency (say, CAD). The bank that processes the payment and the one that handles settlement may be in completely different countries. Because of this, foreign exchange regulations and anti-money-laundering laws prevent Stripe from directly converting and remitting funds ‚Äî it must route them through local partner banks. To fully open local payments in Korea, Stripe can't just flip a switch; it has to partner with Korean banks and card networks while ensuring its overseas settlement rails align. And since the Korean won isn't a fully free-floating international currency, those connections take extra time and red tape. In short, for Stripe to truly make ‚Äúlocal currency in, foreign currency out‚Äù seamless, both domestic and international banking infrastructures have to work together.</p>

<p>We've known about the DCC issue for a long time, so we've already priced our KRW courses lower than their USD or CAD equivalents to offset it. Still, it's a little awkward when the price you see on-screen and the final charged amount don't perfectly match. To make things more confusing, whether DCC is applied depends on the card issuer ‚Äî some cards add it, others don't. Since it varies by card network and policy, customers can't really predict what will happen until they see the charge.</p>

<p>So for now, it's a waiting game. We've waited this long, so waiting a little more isn't a big deal.</p>

<p>There's even an ironic twist. Some people have managed to beat DCC losses by stacking overseas-payment point-back promotions ‚Äî for example, when Naver Pay offered cash-back on international transactions. In a few cases, the rewards outweighed the DCC markup, turning what should've been a loss into a profit. It's not a structural fix, just a lucky promotion timing, but still amusing. Koreans are good at finding these little loopholes.</p>

<p>To sum up, Stripe has officially started supporting Korean local payments, and on paper everything's ready. But to actually feel it in practice, Stripe still needs to complete its settlement infrastructure with Korean partner banks. Without local banks backing the KRW side, the flow from a KRW payment to an overseas settlement currency can't be fully automated. Once Stripe secures enough local partners and streamlines cross-border settlements, that's when true DCC-free local payments will finally become reality.</p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="stripe" /><category term="payment" /><category term="pocu" /><category term="fintech" /><category term="dev" /><category term="korea" /><summary type="html"><![CDATA[Every time you pay on an overseas website, you've probably seen that little popup asking if you'd like to pay in KRW or USD. That's DCC ‚Äî Dynamic Currency Conversion. It sounds convenient, but in reality it almost always costs you more. The merchant or payment processor uses its own exchange rate and adds a markup on top. So, in most cases, it's much cheaper to just pay in the local currency (USD, EUR, etc.). Paying in KRW on a foreign site basically means paying extra for nothing.]]></summary></entry><entry><title type="html">AWS Went Down? Multi-Cloud Isn&apos;t the Answer</title><link href="https://blog.popekim.com/en/2025/10/23/aws-outage-multi-cloud.html" rel="alternate" type="text/html" title="AWS Went Down? Multi-Cloud Isn&apos;t the Answer" /><published>2025-10-23T00:00:00+00:00</published><updated>2025-10-23T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/10/23/aws-outage-multi-cloud</id><content type="html" xml:base="https://blog.popekim.com/en/2025/10/23/aws-outage-multi-cloud.html"><![CDATA[<p>Over the past few days, many people have lost their illusion of "safety."</p>

<p>On October 20 (local time), AWS's US-EAST-1 region suffered a massive outage.</p>

<p>Countless apps and services went down one after another. The cause was traced to DNS resolution failures and issues that originated in internal subsystems and data layers (such as EC2 and DynamoDB APIs).<br />
Social media, gaming, productivity tools‚Äîeven parts of government and education systems‚Äîwere shaken. It took nearly an entire day to recover, and the ripple effects lingered.</p>

<!--more-->

<p>The very next day, a service our company uses on Azure started slowing down, flooding our office with alerts. (For reference: whenever we have a server outage, a disco ball spins and we have an impromptu dance party. Red, blue, dance-dance~) Come to think of it, there was also a large-scale Azure Front Door issue earlier this month (October 9), which even affected the management portal. Microsoft's own status page said they mitigated it by rerouting traffic and purging caches.<br />
Clearly, this isn't just a "single-vendor" problem.</p>

<p>And of course, today's news articles, blogs, and think-pieces are full of the same claim:</p>

<p><strong>"Multi-cloud is safer."</strong></p>

<p>It sounds convincing‚Äîbut in reality, multi-vendor setups can't fix fundamental control-plane (CP) issues or upstream dependencies like global DNS, identity systems, or routing. Even Wall Street touts multi-cloud as the answer, yet that doesn't change the fact that <strong>AWS itself was shaking across the board on that very same day.</strong></p>

<p>My point is simple: <strong>"Cloud means safer" is a myth.</strong><br />
Let's break it down again‚Äîsomething I've said countless times on my YouTube lives.</p>

<h2 id="1-multiply-the-slas-and-youll-see-reality">1) Multiply the SLAs and you'll see reality</h2>

<p>Let's say you run one web server and one database.<br />
(That's the bare minimum architecture, right?)</p>

<ul>
  <li>SLA 99.9% √ó 99.9% = 99.8001%<br />
‚Üí <strong>About 17.5 hours of downtime per year</strong> (0.1999% √ó 8760h)</li>
  <li>SLA 99.9% √ó 99.9% √ó 99.9% = 99.7003%<br />
‚Üí <strong>About 26.3 hours of downtime per year</strong></li>
</ul>

<p>"Serverless means I'm fine," you say?</p>

<p><strong>Serverless still has servers.</strong><br />
The name is just marketing. In the end, it all depends on the reliability of physical and virtual resources, control planes, and rollout mechanisms. Unless you believe in "chicken-free chicken," you can't escape the <strong>multiplication of failure probabilities.</strong></p>

<h2 id="2-the-real-risk-isnt-hardware--its-software-changes">2) The real risk isn't hardware ‚Äî it's software changes</h2>

<p>Cloud providers love to talk about high availability stories like<br />
"Even if one AZ goes down, we'll stay up." That's about <strong>hardware and facilities.</strong></p>

<p>But the truth is, <strong>most major outages start from software or control-plane changes.</strong> If a release goes wrong, healthy hardware can receive <strong>bad configuration at scale</strong>, and multiple regions or services can collapse at once. The latest AWS incident? It started with <strong>a shared dependency</strong> between data paths and DNS resolution‚Äîclassic domino effect.</p>

<h2 id="3-the-vendor-doesnt-deploy-on-my-timeline">3) "The vendor doesn't deploy on my timeline"</h2>

<p>The fundamental risk of the cloud is the <strong>loss of change control.</strong></p>

<p>Vendors push rolling updates <strong>on their schedule</strong>, and you have almost no authority to verify release quality. If your relatively simple service breaks because of a cloud update, it means something broke at the <strong>baseline feature level</strong>‚Äî<strong>(which means the vendor didn't test it well enough).</strong>  And guess who pays for it? Your service.</p>

<h2 id="4-the-forgotten-advantages-of-on-prem--colocation">4) The forgotten advantages of on-prem / colocation</h2>

<ul>
  <li><strong>You control when to update.</strong><br />
No surprise 3 AM deployments behind your back.</li>
  <li>If something breaks mid-deploy, <strong>you can roll back or hot-fix immediately.</strong></li>
  <li><strong>A human (with skills) is right there</strong> to respond in real time.</li>
</ul>

<p>It's like performing surgery <strong>without blood reserves ready.</strong> No matter how great the hospital (cloud) infrastructure is, if your team has no control over the contingency plan, you're in danger.</p>

<h2 id="5-multi-cloud-is-the-answer--that-sounded-cool-back-when-we-were-newbies">5) "Multi-cloud is the answer"? ‚Äî That sounded cool back when we were newbies.</h2>

<p>Multi-cloud overlooks two big realities:</p>

<ol>
  <li><strong>Data Gravity</strong><br />
Keeping transactional data consistently replicated and fail-over-ready across clouds is a nightmare of latency, consistency, and locking strategies.</li>
  <li><strong>Shared dependencies</strong><br />
Global DNS, identity, SaaS CI/CD, logging, alerts, CDNs, version control‚Äîdifferent vendors, yes, but <strong>shared upper-layer dependencies</strong> mean they can all fall together.</li>
</ol>

<p>Sure, for certain workloads‚Äîread-only caches, content delivery, or non-critical back-office tasks‚Äî multi-vendor setups can reduce risk.<br />
But for <strong>core transactional paths</strong>, multi-cloud often <strong>increases complexity, cost, and the blast radius</strong> of outages.</p>

<h3 id="so-how-can-we-be-safer">So how can we be "safer"?</h3>

<p>This isn't a "ditch the cloud" rant. Cloud is fantastic for <strong>initial launches and experiments.</strong> But once your product hits a stable phase, consider the following:</p>

<ol>
  <li><strong>Escape single-region, minimize single control plane</strong><br />
Even within the same vendor, use <strong>region/account separation</strong> to reduce blast radius. Split control and data planes (e.g., separate management/audit accounts).</li>
  <li><strong>The courage to use "boring" tech</strong><br />
Simplify core systems with proven components instead of shiny new managed ones. Migrate in controlled, incremental phases.</li>
  <li><strong>Strict change management</strong><br />
Canary releases, circuit breakers, feature flags, graceful degradation‚Äînon-negotiable. Align your own release calendar with vendor change windows and avoid peak-time changes.</li>
  <li><strong>Off-cloud backstops</strong><br />
Provide read-only static or cached fail-safes (e.g., critical notice pages, cached order history) accessible through alternate paths.</li>
  <li><strong>Real-world game days</strong><br />
Your team should physically rehearse failure scenarios‚ÄîDNS down, identity down, storage down, message broker down‚Äî<br />
and internalize the runbook, RTO, and RPO.</li>
  <li><strong>Mature-phase strategy: Hybrid / On-prem relocation</strong><br />
Bring <strong>core transactional and stateful tiers</strong> on-prem or to colocation. Keep <strong>edge, burst, and analytics</strong> in the cloud.<br />
The cloud should become <strong>a tool you use when needed</strong>, not the place you live in.</li>
</ol>

<h2 id="in-summary-assuming-people-never-make-mistakes-is-the-real-danger">In summary: assuming "people never make mistakes" is the real danger</h2>

<p>The cloud is a great tool. I still happily use it for <strong>early-stage products.</strong></p>

<p>But once a service stabilizes, we should <strong>reduce cloud dependency</strong> and take back control of our core systems. That's what creates <strong>real safety.</strong></p>

<p>It's not <strong>"Cloud = Safe."</strong><br />
It's <strong>"Controlled change + Verifiable design = Safe."</strong></p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="cloud" /><category term="defensive-programming" /><category term="server" /><category term="dev" /><category term="rants" /><summary type="html"><![CDATA[Over the past few days, many people have lost their illusion of "safety." On October 20 (local time), AWS's US-EAST-1 region suffered a massive outage. Countless apps and services went down one after another. The cause was traced to DNS resolution failures and issues that originated in internal subsystems and data layers (such as EC2 and DynamoDB APIs). Social media, gaming, productivity tools‚Äîeven parts of government and education systems‚Äîwere shaken. It took nearly an entire day to recover, and the ripple effects lingered.]]></summary></entry><entry><title type="html">Rust Is a Great Language ‚Äî But It&apos;s Not a Religion</title><link href="https://blog.popekim.com/en/2025/10/22/rust-is-not-a-religion.html" rel="alternate" type="text/html" title="Rust Is a Great Language ‚Äî But It&apos;s Not a Religion" /><published>2025-10-22T00:00:00+00:00</published><updated>2025-10-22T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/10/22/rust-is-not-a-religion</id><content type="html" xml:base="https://blog.popekim.com/en/2025/10/22/rust-is-not-a-religion.html"><![CDATA[<p>Ten years ago, when no one cared, I was already saying it: <strong>"Rust is a great language."</strong></p>

<p>Back then, I had no data to back it up. It was just my gut feeling and experience. Rust was designed in a way that naturally prevents programmers from making common mistakes.</p>

<!--more-->

<p>And now, the data proves it. Microsoft, Google, and Android have all confirmed that over half of security vulnerabilities come from memory safety issues. But with Rust, such mistakes are <strong>impossible by design.</strong> In real-world Rust code, those vulnerabilities have dramatically decreased.</p>

<h2 id="the-fatigue-of-the-rust-religion">The Fatigue of the Rust Religion</h2>

<p>But honestly, I'm getting tired of the way people talk about Rust these days. You hear things like, "Rust will kill every other language," or "C++ is dead."</p>

<p>That's not a technical discussion ‚Äî that's <strong>religion.</strong> There's no logic, no data. It's just people with little skill trying to sound relevant by jumping on the "innovation" bandwagon. And ironically, this kind of hype actually hurts Rust's progress.</p>

<h2 id="why-c-is-still-alive">Why C++ Is Still Alive</h2>

<p>People often say Rust will replace C++. So let me ask ‚Äî why isn't C++ dead yet?</p>

<ol>
  <li>
    <p><strong>There's simply too much legacy code.</strong> Game engines, operating systems, native libraries, embedded systems ‚Äî decades of C++ code power the world. Rewriting all of that in Rust overnight is impossible.</p>
  </li>
  <li>
    <p><strong>The tools and ecosystem are incredibly strong.</strong> Just look at Visual Studio. Back in the day, even Sony and Nintendo had their own IDEs. But as codebases grew and development efficiency became critical, both ended up supporting Visual Studio. The result? The entire industry moved under the C++ ecosystem. Instead of dying, C++ actually became stronger.</p>
  </li>
  <li>
    <p><strong>The learning curve and talent pool.</strong> Rust enforces strict safety guarantees, but that also means fewer developers can handle it well. Meanwhile, C++ still has a massive developer base and a deeply established presence in both academia and industry. It's not something Rust can replace overnight.</p>
  </li>
  <li>
    <p><strong>Other languages can adopt Rust's innovations.</strong> Memory safety, concurrency models ‚Äî those can (and will) be borrowed. The "unique innovation" that once defined Rust won't stay exclusive forever.</p>
  </li>
</ol>

<h2 id="safety-alone-wont-make-you-a-better-developer">Safety Alone Won't Make You a Better Developer</h2>

<p>Lastly, there's a limit to developers who have only used "safe" languages. If you've never wrestled with a wild language like C and felt its pain firsthand, you might still write weird, unsafe logic even inside a safe language like Rust.</p>

<p>This isn't new. We saw the same thing years ago with Java and C# ‚Äî managed languages that made developers comfortable, but not necessarily competent.</p>

<h2 id="history-repeats-itself">History Repeats Itself</h2>

<p>Remember when Java once declared, "C++ is over! Java will rule the world"?</p>

<p>And what happened? C++ is still alive and well, while Java is now worried about losing its market share. Rust could fall into the same trap if it's not careful. Rust is a great language, but <strong>turning it into a religion will destroy it.</strong></p>

<h2 id="conclusion-a-language-is-just-a-tool">Conclusion: A Language Is Just a Tool</h2>

<p>When evaluating a language, you must separate objectivity from subjectivity. Objectivity is about whether the language <strong>actually reduces human error</strong> ‚Äî and whether that's <strong>proven by data.</strong> By that measure, Rust is an excellent language.</p>

<p>But saying "I like using it" is purely subjective. And once you start presenting that as objective truth, technology disappears and religion takes its place.</p>

<p><strong>Programming languages are not religions.</strong> They are tools. And tools should always be used according to data and reality.</p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="rust" /><category term="cpp" /><category term="dev" /><category term="rants" /><summary type="html"><![CDATA[Ten years ago, when no one cared, I was already saying it: "Rust is a great language." Back then, I had no data to back it up. It was just my gut feeling and experience. Rust was designed in a way that naturally prevents programmers from making common mistakes.]]></summary></entry><entry><title type="html">How to Minimize Side Effects When Writing to Two Databases at the Same Time</title><link href="https://blog.popekim.com/en/2025/10/17/two-db-commit-side-effects.html" rel="alternate" type="text/html" title="How to Minimize Side Effects When Writing to Two Databases at the Same Time" /><published>2025-10-17T00:00:00+00:00</published><updated>2025-10-17T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/10/17/two-db-commit-side-effects</id><content type="html" xml:base="https://blog.popekim.com/en/2025/10/17/two-db-commit-side-effects.html"><![CDATA[<p>Normally, data is stored in a single database.<br />
However, sometimes you may need to write to <strong>two physically separate DB servers</strong> at the same time.</p>

<h2 id="the-naive-approach-everyone-starts-with">The naive approach everyone starts with</h2>

<p>The examples in this post are written in C# using EF Core.</p>

<pre><code class="language-csharp">await mDbContext0.SaveChangesAsync();
await mDbContext1.SaveChangesAsync();
</code></pre>

<p>Most people simply write this way.<br />
But this <em>will definitely</em> break one day.</p>

<p>Right after the first commit completes,</p>
<ul>
  <li>The second DB's <code>SaveChangesAsync()</code> might fail due to a validation error or constraint violation,</li>
  <li>The network could go down, or</li>
  <li>A power failure could occur.</li>
</ul>

<p>In the end, only the first DB is updated, and the second one fails.<br />
In other words, you get <strong>a partially committed state</strong>, resulting in <strong>data inconsistency</strong>.</p>

<h2 id="how-i-solved-it-today">How I solved it today</h2>

<p>‚ö†Ô∏è This approach is not perfect.</p>

<pre><code class="language-csharp">private async Task crossCommitBestEffortAsync()
{
    await using (IDbContextTransaction tx0 = await mDbContext0.Database.BeginTransactionAsync())
    await using (IDbContextTransaction tx1 = await mDbContext1.Database.BeginTransactionAsync())
    {
        // best-effort attempt to make two independent DB commits look atomic
        // still unsafe if:
        //   1) tx0.CommitAsync() succeeds, and
        //   2) power failure happens before tx1.CommitAsync()
        try
        {
            await mDbContext0.SaveChangesAsync();
            await mDbContext1.SaveChangesAsync();

            await tx0.CommitAsync();
            await tx1.CommitAsync();
        }
        catch
        {
            await tx0.RollbackAsync();
            await tx1.RollbackAsync();
            throw;
        }
    }
}
</code></pre>

<p>This code opens <strong>a separate transaction for each DB</strong><br />
and only commits if both <code>SaveChangesAsync()</code> calls succeed.</p>

<h2 id="whats-different-from-before">What's different from before?</h2>

<p>In the naive version (<code>SaveChangesAsync()</code> twice),<br />
if the first commit succeeds and the second throws an exception,<br />
<strong>there's no way to revert the already committed data.</strong></p>

<p>In contrast, this code:</p>
<ul>
  <li>Rolls back <strong>both transactions</strong> if either <code>SaveChangesAsync()</code> or <code>CommitAsync()</code> fails.</li>
  <li>Ensures that under <strong>normal execution flow</strong>, both DBs either commit or roll back together.</li>
</ul>

<p>This is a <strong>‚Äúbest effort‚Äù</strong> approach ‚Äî<br />
as long as the OS and process stay alive, both DBs will end in the same state.</p>

<h2 id="why-its-still-not-perfect">Why it's still not perfect</h2>

<p>The real problem is <strong>physical failure</strong>.<br />
In other words, the code handles logic-level consistency, but not system-level reliability.</p>

<p>For example, the following sequence will break things üëá</p>
<ol>
  <li><code>tx0.CommitAsync()</code> succeeds</li>
  <li>A power outage or process crash occurs</li>
  <li><code>tx1.CommitAsync()</code> never gets called</li>
</ol>

<p>Now, DB0 has committed while DB1 has not.<br />
The two databases are out of sync.</p>

<p>There's no way to prevent this in code,<br />
because the two DBs live on <strong>independent physical servers.</strong></p>

<h2 id="not-suitable-for-mission-critical-systems">Not suitable for mission-critical systems</h2>

<p>Although the time gap between commits is small,<br />
‚Äúsmall‚Äù doesn't mean <strong>simultaneous</strong>.</p>

<blockquote>
  <p>If a power failure happens 0.001 seconds after the first commit, the data becomes inconsistent.</p>
</blockquote>

<p>Therefore, this approach should <strong>never</strong> be used in mission-critical transactions<br />
such as payments, settlements, or order processing.</p>

<h2 id="why-i-still-used-it">Why I still used it</h2>

<p>This pattern was used in an <strong>internal developer tool</strong>,<br />
not in a public-facing service.</p>

<p>The chance of failure was extremely low,<br />
and even if it did happen, the <strong>developer</strong> was actively using the tool<br />
and could immediately detect and correct the issue.</p>

<p>In short, it was acceptable in this <strong>low-risk environment</strong>.</p>

<h2 id="the-proper-way-use-a-message-queue">The proper way: use a message queue</h2>

<p>To handle this safely, you should use a <strong>Message Queue (MQ)</strong>.</p>

<p>However, the naive approach ‚Äî committing to the DB first and then pushing a message ‚Äî is still unsafe.<br />
If the system crashes or loses power right after the DB commit, the message never gets queued,<br />
and you lose the chance to reprocess it.</p>

<p>A more reliable approach is to <strong>push every update request to the queue first</strong>,<br />
and then let the <strong>consumer</strong> update DB0 and DB1 in sequence.</p>

<p>This way, the producer only performs one action ‚Äî sending a message ‚Äî<br />
and even if a failure occurs, the pending message can always be <strong>reprocessed</strong> later.</p>

<p>You'll still need mechanisms like <strong>deduplication</strong> and <strong>pre-validation</strong>,<br />
but at least you won't end up with inconsistent data.</p>

<p>If the consumer can't update the second DB<br />
because of a validation error or business rule violation,<br />
then a <strong>compensating transaction</strong> must be applied to the first DB<br />
to roll back the previous change.</p>

<p>So even with a queue-based design, it's not fully automatic ‚Äî<br />
you still need explicit rollback logic to keep the system consistent.</p>

<p>Keep in mind that adopting an MQ means<br />
<strong>adding another program to run and another data store to maintain.</strong><br />
It introduces an extra operational layer,<br />
and debugging becomes harder since you can't simply inspect it with SQL.</p>

<p>In short, reliability increases, but so does complexity.<br />
If your system is small or failures can be handled manually,<br />
you may not need an MQ.<br />
But if <strong>stability is your top priority</strong>, this is the right direction to go.</p>

<p>When starting out, I recommend using a <strong>Rebus + SQL</strong> combo<br />
instead of a complex distributed MQ system ‚Äî<br />
it's easy to configure and supports transactional consistency cleanly.</p>

<h2 id="note-msdtc-works-only-on-premises">Note: MSDTC works only on-premises</h2>

<p>If you're running Windows servers on-premises,<br />
you can use <strong>MSDTC (Microsoft Distributed Transaction Coordinator)</strong><br />
to coordinate fully atomic distributed transactions across multiple databases.</p>

<pre><code class="language-csharp">using (var scope = new TransactionScope(TransactionScopeAsyncFlowOption.Enabled))
{
    await mDbContext0.SaveChangesAsync();
    await mDbContext1.SaveChangesAsync();
    scope.Complete();
}
</code></pre>

<p>This guarantees atomic commits.<br />
However, <strong>Azure SQL Database does not support MSDTC.</strong></p>

<p>So in cloud environments, you're limited to either<br />
<strong>best-effort commits</strong> or <strong>queue-based compensating transactions.</strong></p>

<h2 id="conclusion">Conclusion</h2>
<ol>
  <li>Calling <code>SaveChangesAsync()</code> twice will eventually cause trouble.</li>
  <li><code>crossCommitBestEffortAsync()</code> keeps things consistent in normal scenarios,<br />
but it's still vulnerable to physical failures like power loss.</li>
  <li>It's not suitable for mission-critical systems.</li>
  <li>For safety, you need a <strong>queue-based design.</strong></li>
  <li><strong>MSDTC works only on-premises.</strong></li>
</ol>

<p>In the end, ‚Äútwo commits‚Äù will betray you, but <strong>a queue will save your system.</strong> üòè</p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="csharp" /><category term="database" /><category term="defensive-programming" /><category term="ef-core" /><category term="transaction" /><category term="distributed-transaction" /><category term="dev" /><category term="dev-diary" /><summary type="html"><![CDATA[Normally, data is stored in a single database. However, sometimes you may need to write to two physically separate DB servers at the same time.]]></summary></entry><entry><title type="html">Five Defensive Utility Functions I Made</title><link href="https://blog.popekim.com/en/2025/10/11/defensive-assertion-utils.html" rel="alternate" type="text/html" title="Five Defensive Utility Functions I Made" /><published>2025-10-11T00:00:00+00:00</published><updated>2025-10-11T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/10/11/defensive-assertion-utils</id><content type="html" xml:base="https://blog.popekim.com/en/2025/10/11/defensive-assertion-utils.html"><![CDATA[<h2 id="-introduction">üéØ Introduction</h2>

<p>The built-in <code>Debug.Assert()</code> in C# just wasn't enough. I wanted a unified system to handle <strong>assumption violations, internal bugs, and critical runtime issues that require immediate attention</strong> in a consistent way.</p>

<p>So, I created the following five utility functions:</p>

<pre><code class="language-csharp">global using static POCU.Core.Assertion.Check;
</code></pre>

<p>By declaring them globally and writing them in <strong>ALL CAPS</strong>, they stand out clearly inside logic code. Even at a glance, you can tell: <em>"This is defensive code."</em></p>

<h2 id="-common-rule">üß± Common Rule</h2>

<p>The <strong>first parameter of every function must be a <code>bool</code> expression.</strong></p>

<pre><code class="language-csharp">CHECK911_THROW(user != null, "User should not be null");
</code></pre>

<ul>
  <li>If the expression is <code>true</code> ‚Üí nothing happens.</li>
  <li>If <code>false</code> ‚Üí the corresponding function triggers <strong>logging, alerts, or exceptions</strong> depending on its level.</li>
</ul>

<p>In other words, you don't have to keep writing <code>if (!condition) { ... }</code>. A single expression clearly declares, <em>"If this breaks, something is wrong."</em></p>

<h2 id="-function-summary">üìä Function Summary</h2>

<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>Purpose</th>
      <th>Behavior</th>
      <th>Ops Team Response</th>
      <th>Exception</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>VERIFY</strong></td>
      <td>Observe "this should never happen" assumptions</td>
      <td>Logs only, continues execution</td>
      <td>Check later</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td><strong>DBG_CHECK</strong></td>
      <td>Debug-only assertion</td>
      <td>Stops in Debug builds only</td>
      <td>‚ùå</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td><strong>CHECK912</strong></td>
      <td>Detect internal issue (non-urgent)</td>
      <td>Alerts + metrics tracking</td>
      <td>Review/fix within 1‚Äì2 days</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td><strong>CHECK911</strong></td>
      <td>Requires immediate attention</td>
      <td>Alerts + metrics tracking</td>
      <td>Immediate fix</td>
      <td>‚ùå</td>
    </tr>
    <tr>
      <td><strong>CHECK911_THROW</strong></td>
      <td>Transaction protection</td>
      <td>Alerts + metrics tracking + throws exception</td>
      <td>Immediate fix</td>
      <td>‚úÖ</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>üîî All CHECK functions send notifications to our company messenger, and their occurrences are automatically <strong>tracked and visualized on dashboards.</strong></p>
</blockquote>

<h2 id="-verify--long-term-assumption-monitor">üß© VERIFY ‚Äî Long-term Assumption Monitor</h2>

<p><code>VERIFY</code> is more than a simple log statement. It's a <strong>sensor for assumptions</strong> ‚Äî things you believe "should never happen," monitored over time.</p>

<pre><code class="language-csharp">VERIFY(order.TotalPrice &gt;= 0, "Order total should never be negative");
</code></pre>

<ul>
  <li>You believe this condition will never fail.</li>
  <li>But maybe, someday, once in two years, it might.</li>
  <li>When it does, a log arrives ‚Äî and you realize your assumption was wrong.</li>
  <li>If it never triggers for years, you can safely remove that line.</li>
</ul>

<p>This is <strong>runtime validation for assumptions that can't be tested easily</strong>. It's perfect for monitoring rare edge cases or unexpected system behavior in production.</p>

<h3 id="-one-more-trick">üí° One More Trick</h3>

<p>Sometimes you encounter old code that <em>should never happen anymore</em>, but you're not 100% sure it's safe to delete. In that case, do this:</p>

<pre><code class="language-csharp">// TODO(delete): delete after 2028-12-31
VERIFY(someOldFlag == false, "Old flag still being used?");
</code></pre>

<ul>
  <li>Add a <code>VERIFY</code>.</li>
  <li>Watch it in production for a year.</li>
  <li>If it never triggers, safely delete the code.</li>
</ul>

<p>This pattern works like a <strong>"live test" in production</strong>. It's like having thousands of live users running an <strong>automatic test system for free. Amazing, right? üòé</strong> It also makes long-term code cleanup much easier to manage.</p>

<h2 id="-dbg_check--debug-only-strong-assertion">üß™ DBG_CHECK ‚Äî Debug-only Strong Assertion</h2>

<pre><code class="language-csharp">DBG_CHECK(buffer.Length == expectedSize, "Unexpected buffer size");
</code></pre>

<ul>
  <li>Executes <strong>only in Debug builds</strong>.</li>
  <li>Completely removed in Release builds, with zero runtime overhead.</li>
  <li>Enforces conditions that "must never fail during development."</li>
</ul>

<blockquote>
  <p>‚úÖ Use <code>DBG_CHECK</code> for conditions that must break during testing.<br />
‚úÖ Use <code>VERIFY</code> for conditions you want to observe in production.</p>
</blockquote>

<h2 id="-check912--internal-issue-non-urgent">üîç CHECK912 ‚Äî Internal Issue (Non-Urgent)</h2>

<pre><code class="language-csharp">CHECK912(userCache.Count &gt; 0, "User cache is unexpectedly empty");
</code></pre>

<ul>
  <li>Indicates a likely internal bug, but <strong>no immediate user impact.</strong></li>
  <li>Sends alerts and updates metrics.</li>
  <li>Ops team handles it within a day or two.</li>
</ul>

<p>Examples:</p>
<ul>
  <li>Cache desynchronization</li>
  <li>Retried network failure that succeeded later</li>
  <li>Minor data anomalies</li>
</ul>

<h2 id="-check911--critical-requires-immediate-action">üö® CHECK911 ‚Äî Critical, Requires Immediate Action</h2>

<pre><code class="language-csharp">CHECK911(paymentResponse.IsValid, "Payment gateway returned invalid data");
</code></pre>

<ul>
  <li>Used for <strong>critical situations</strong> like data loss, security issues, or direct customer impact.</li>
  <li><strong>Sends instant alerts</strong> and updates metrics/dashboards.</li>
  <li>May also trigger operational hooks (e.g., safe mode switch).</li>
  <li>Does <strong>not</strong> throw exceptions, so background workers or pipelines can keep running while the ops team investigates.</li>
</ul>

<h2 id="-check911_throw--transaction-protection">üí£ CHECK911_THROW ‚Äî Transaction Protection</h2>

<pre><code class="language-csharp">CHECK911_THROW(invoice != null, "Invoice must exist before commit");
</code></pre>

<ul>
  <li>Behaves exactly like <code>CHECK911</code>, but also <strong>throws an exception</strong> to abort the current transaction.</li>
  <li>Prevents corrupted state from propagating to databases or external systems.</li>
  <li>Typically used at transaction boundaries, commit points, or right after external API calls.</li>
</ul>

<p>Examples:</p>
<pre><code class="language-csharp">CHECK911_THROW(user != null, "User not found");
CHECK911_THROW(balance &gt;= 0, "Negative balance detected");
</code></pre>

<blockquote>
  <p>Use it right before committing, or immediately after a risky side effect ‚Äî it stops the error from spreading further.</p>
</blockquote>

<h2 id="Ô∏è-internal-mechanism-overview">‚öôÔ∏è Internal Mechanism Overview</h2>

<p>All functions share the same underlying structure:</p>

<ol>
  <li>Evaluate the <code>bool</code> expression.</li>
  <li>If <code>false</code>, call a shared logger.</li>
  <li>The logger does the following:
    <ul>
      <li>Write logs (file, console, Sentry, etc.)</li>
      <li>Send messenger notifications</li>
      <li>Update metrics for dashboards</li>
    </ul>
  </li>
  <li><code>*_THROW</code> variants also perform <code>throw</code> at the end.</li>
</ol>

<p>In short, all of them use the same alerting infrastructure ‚Äî they just differ in severity and whether they stop execution.</p>

<h2 id="-conclusion">üß© Conclusion</h2>

<p>All five functions serve one purpose:<br />
<strong>"Separate logic from defense, and respond appropriately depending on when and how a problem occurs."</strong></p>

<ul>
  <li><code>VERIFY</code>: Long-term assumption monitor</li>
  <li><code>DBG_CHECK</code>: Debug-only strong assertion</li>
  <li><code>CHECK912</code>: Internal issue detection (non-urgent)</li>
  <li><code>CHECK911</code>: Critical issue (immediate attention)</li>
  <li><code>CHECK911_THROW</code>: Critical + transaction abort</li>
</ul>

<p>And in practice, all you need in your logic code is one line:</p>

<pre><code class="language-csharp">CHECK911_THROW(totalPrice &gt;= 0, "Negative total price detected");
</code></pre>

<p>That's it. Anyone reading it immediately knows: this isn't regular logic ‚Äî <strong>it's a safety line.</strong></p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="csharp" /><category term="assertion" /><category term="debugging" /><category term="software-engineering" /><category term="defensive-programming" /><summary type="html"><![CDATA[üéØ Introduction]]></summary></entry><entry><title type="html">Engineering in Plain Sight Review: A New Way of Seeing the World</title><link href="https://blog.popekim.com/en/2025/09/02/engineering-in-plain-sight-review.html" rel="alternate" type="text/html" title="Engineering in Plain Sight Review: A New Way of Seeing the World" /><published>2025-09-02T00:00:00+00:00</published><updated>2025-09-02T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/09/02/engineering-in-plain-sight-review</id><content type="html" xml:base="https://blog.popekim.com/en/2025/09/02/engineering-in-plain-sight-review.html"><![CDATA[<p>One day, while browsing the shelves at the library, I stumbled upon a book that caught my eye. The title was <em>Engineering in Plain Sight</em> (by Grady Hillhouse). Honestly, at first I just thought the illustrations were pretty, and the first few pages looked fun, so I picked it up lightly, assuming it was just some kind of illustrated reference book.</p>

<p>But the more I read, the more surprised I became. It wasn't just a catalog of "roads, bridges, drains," but a book that revealed in a completely different light the things we've <strong>taken for granted since birth, simply because they've always been there.</strong></p>

<p>As I turned the pages, I realized that all of this infrastructure exists thanks to <em>design, labor, and government investment</em>. Before, I would vaguely think, "Managing all these roads and structures must require a lot of taxes. Taxes aren't a waste." But this book made that thought more concrete and real. It's not just that "it costs a lot of money," but I came to understand <strong>the principles, the methods, and the reasons why maintenance is essential.</strong></p>

<h2 id="power-and-communication-networks">Power and Communication Networks</h2>

<p>The early chapters cover <strong>power and communication networks</strong>, and this is where my perspective started to change. The electricity and internet we use daily without a second thought are, in fact, supported by massive infrastructure. What impressed me most was how the story moved from <strong>older communication networks all the way to modern cellular towers.</strong> From the cables strung along utility poles to the silver masts scattered throughout the city, I realized these weren't just random or decorative objects. Suddenly, ordinary scenery began to take on meaning.</p>

<h2 id="roads-and-tunnels">Roads and Tunnels</h2>

<p>The middle chapters focus on <strong>roads and tunnels.</strong> I never realized how much engineering went into the asphalt we drive on every day. Drainage design, paving materials, even the way lane markings are drawn‚Äîall of it has a reason. The tunnel section, with its explanation of ventilation systems, lighting, and fire escape passages, was especially eye-opening. I used to think of tunnels as nothing more than "cramped spaces," but now when I pass through one, I notice the ceiling fans and realize those emergency exits aren't just for show.</p>

<h2 id="water-supply-and-sewage-yes-poop">Water Supply and Sewage (Yes, Poop!)</h2>

<p>My personal favorite section was the one about <strong>water supply and sewage‚Äîyes, the poop chapter!</strong> I always thought clean water came out of the tap and wastewater just went "down the drain," but behind that is centuries of accumulated engineering. The book's diagrams of pipes, treatment processes, and purification facilities made me realize, "Wow, it's thanks to this complex and carefully designed system that we can live comfortably and safely." Honestly, it was so fascinating that I found myself chuckling while reading. <em>"This is the hidden world of poop!"</em> üòÇ</p>

<h2 id="construction">Construction</h2>

<p>The final chapter (Chapter 8) covers <strong>construction</strong>, and this part felt a little underwhelming. Heavy machinery and construction sites are interesting in their own right, but coming right after the unforgettable sewage chapter, it felt less impactful by comparison. Not bad, but for me, Chapter 7 was so strong that Chapter 8 felt more like a bonus.</p>

<h2 id="closing-the-book">Closing the Book</h2>

<p>After finishing this book, my perspective has definitely changed. Where before I only vaguely thought "this must cost a lot of taxes," now I connect what I see with the <strong>specific systems and human effort</strong> behind it all. When I walk down the street, manhole covers, traffic lights, drains, and cell towers all catch my eye, and I think, "Ah, that's exactly what I saw in the book." It's become a private little joy. Others might pass by without noticing, but now I know the meaning‚Äîand that's surprisingly fun.</p>

<p>More than anything, this book gave me a sense of <strong>humility and gratitude.</strong> From the unseen sewage pipes carrying waste to the steel frameworks that hold up our cities, it's thanks to the expertise and hard work of countless people that I can live safely and comfortably each day.</p>

<p>That's why I want to recommend this book especially to developers like me, who spend our lives tapping on keyboards. Don't just stare at the virtual world inside a computer. Turn your eyes to the real world beneath your feet. Step away from the "shallow perspective of a software engineer" and, at least for a moment, adopt the mindset of a <strong>civil engineer hauling sewage</strong> to keep the city running. That is the greatest gift this book has to offer.</p>

<p>After all, we live in the <strong>real world, don't we?</strong></p>

<p>So I can say this with confidence:</p>

<p>üëâ <em>Engineering in Plain Sight</em> ‚Äî definitely worth a read.</p>]]></content><author><name>Pope Kim</name></author><category term="personal" /><category term="book" /><summary type="html"><![CDATA[One day, while browsing the shelves at the library, I stumbled upon a book that caught my eye. The title was Engineering in Plain Sight (by Grady Hillhouse). Honestly, at first I just thought the illustrations were pretty, and the first few pages looked fun, so I picked it up lightly, assuming it was just some kind of illustrated reference book.]]></summary></entry><entry><title type="html">In an Era of Frequent Facebook Graph API Deprecations, This Is How You Manage ASP.NET Core Login</title><link href="https://blog.popekim.com/en/2025/08/31/aspnet-core-facebook-login-graph-api-management.html" rel="alternate" type="text/html" title="In an Era of Frequent Facebook Graph API Deprecations, This Is How You Manage ASP.NET Core Login" /><published>2025-08-31T00:00:00+00:00</published><updated>2025-08-31T00:00:00+00:00</updated><id>https://blog.popekim.com/en/2025/08/31/aspnet-core-facebook-login-graph-api-management</id><content type="html" xml:base="https://blog.popekim.com/en/2025/08/31/aspnet-core-facebook-login-graph-api-management.html"><![CDATA[<p>Adding Facebook login to ASP.NET Core is supposed to be really simple. Just install the <code>Microsoft.AspNetCore.Authentication.Facebook</code> NuGet package, call <code>services.AddFacebook()</code>, set the AppId and Secret, and you're done. The login screen appears, and tokens come back as expected.</p>

<p>But if you trust this setup blindly, one day your service may suddenly stop working. I actually got burned by this a few years ago. The reason was simple: <strong>the default Facebook Graph API version hardcoded in the NuGet package was too old</strong>. At the time it was still using v11, and once Facebook officially ended support for that version, login broke immediately.</p>

<h2 id="why-do-i-care-so-much">Why do I care so much?</h2>

<p>At POCU Academy, we <strong>rely solely on social logins</strong>. We never store passwords or unnecessary personal data like social security numbers. Storing passwords is much riskier than many people think. Even if you encrypt and protect them well, leaks always remain a possibility. Most security incident headlines you see follow the same pattern: "our service DB was hacked ‚Üí user passwords leaked."</p>

<p>Instead, we delegate <strong>all authentication to external providers like Facebook, Google, or Naver</strong>, and only consume the authenticated result. For us, it's basically just a signed certificate saying "this person really is who they claim to be."</p>

<p>Concretely, after a successful Facebook login, we typically receive values like:</p>

<ul>
  <li><strong>Provider key</strong>: a unique key Facebook issues per user</li>
  <li><strong>Access token</strong>: the token used to access the Facebook Graph API</li>
  <li>(optional) <strong>Email, name</strong>, or other basic profile info</li>
</ul>

<p>That's all we need to confirm the identity. From our perspective it's far safer. We never touch "sensitive user passwords," and the responsibility lies with the external provider.</p>

<h2 id="wait-how-does-oauth-work-again">Wait, how does OAuth work again?</h2>

<p>Facebook login is powered by the <strong>OAuth 2.0 protocol</strong>. It can be complicated, but simplified it looks like this:</p>

<ol>
  <li>The user clicks "Login with Facebook."</li>
  <li>Our service redirects the user to the Facebook login page.</li>
  <li>The user enters their Facebook credentials and approves access.</li>
  <li>Facebook redirects back to our service with an <strong>Authorization Code</strong>.</li>
  <li>Our service exchanges that code with Facebook for an <strong>Access Token</strong>.</li>
  <li>With that token, we call the <code>/me</code> API to fetch the user profile.</li>
</ol>

<p>The key point is that our service <strong>never needs to know the user's password directly</strong>. Facebook handles all authentication, and we only trust the "token" they issue.</p>

<h2 id="the-nuget-package-problem">The NuGet package problem</h2>

<p>Here's the catch: the Facebook Graph API version used in this process is not always the latest. The <code>Microsoft.AspNetCore.Authentication.Facebook</code> package hardcodes its internal endpoints. For example, it used to default to v11, and once that version hit EOL, services broke overnight.</p>

<p>So I just override the endpoints directly:</p>

<pre><code class="language-csharp">services.AddAuthentication()
    .AddFacebook(facebookOptions =&gt;
    {
        facebookOptions.AuthorizationEndpoint = Constants.AUTHORIZATION_ENDPOINT;
        facebookOptions.UserInformationEndpoint = Constants.USER_INFORMATION_ENDPOINT;
        facebookOptions.TokenEndpoint = Constants.TOKEN_ENDPOINT;

        facebookOptions.AppId = config["Authentication:Facebook:AppId"];
        facebookOptions.AppSecret = config["Authentication:Facebook:AppSecret"];
        facebookOptions.AccessDeniedPath = accountAccessDeniedPath;
        facebookOptions.Events.OnRemoteFailure = handleOnRemoteFailureAsync;
    });
</code></pre>

<p>And the constants:</p>

<pre><code class="language-csharp">public static class Constants
{
    public const string AUTHORIZATION_ENDPOINT = "https://www.facebook.com/v17.0/dialog/oauth";

    public const string USER_INFORMATION_ENDPOINT = "https://graph.facebook.com/v17.0/me";

    public const string TOKEN_ENDPOINT = "https://graph.facebook.com/v17.0/oauth/access_token";
}
</code></pre>

<h2 id="why-not-remove-this-override">Why not remove this override?</h2>

<p>Honestly, I'd love to remove these constants someday. If the NuGet package reliably updated to the latest API versions, I wouldn't need to maintain them myself. But reality is different‚Ä¶ <strong>even now, with v17 nearing its deprecation date, the official NuGet release hasn't been updated</strong>. The preview branch already contains the fix, but the stable release still lags behind.</p>

<p>So I don't trust it. At this point, maintaining my own overrides is less stressful. Luckily, Facebook emails developers before an API version reaches EOL. When that happens, I just bump the version string in my constants and run some quick tests. It's basically "manual version management," but it keeps me from waking up to a broken login system.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Facebook retires old versions too frequently, and Microsoft's NuGet package doesn't always keep pace. Developers get caught in between. That's why I keep overriding the endpoints and managing versions myself.</p>

<p><strong>Lesson: ASP.NET Core Facebook login? Don't trust the defaults. Manage the version yourself.</strong></p>]]></content><author><name>Pope Kim</name></author><category term="dev" /><category term="dev" /><category term="best-practice" /><category term="web" /><category term="csharp" /><category term="defensive-programming" /><summary type="html"><![CDATA[Adding Facebook login to ASP.NET Core is supposed to be really simple. Just install the Microsoft.AspNetCore.Authentication.Facebook NuGet package, call services.AddFacebook(), set the AppId and Secret, and you're done. The login screen appears, and tokens come back as expected.]]></summary></entry></feed>